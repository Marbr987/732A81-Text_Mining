{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import time\n",
    "from scripts.prepare_data_helpers import prepare_examples, create_query\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('../../e-SNLI/dataset/esnli_train_1.csv')\n",
    "train2 = pd.read_csv('../../e-SNLI/dataset/esnli_train_2.csv')\n",
    "train = pd.concat([train1, train2])\n",
    "dev = pd.read_csv('../../e-SNLI/dataset/esnli_dev.csv')\n",
    "test = pd.read_csv('../../e-SNLI/dataset/esnli_test.csv')\n",
    "\n",
    "train = train.dropna(subset=['Sentence1', 'Sentence2', 'Explanation_1'])\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "np.random.seed(12345) # seed for numpy package\n",
    "test_indices = list(np.random.choice(test.index.values, size=1000, replace=False))\n",
    "test = test.loc[test_indices]\n",
    "test = test.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "np.random.seed(12345) # seed for numpy package\n",
    "random.seed(12345) # seed for random package\n",
    "prompts = [prepare_examples(train, size_per_class=4, include_label=True) + '\\n\\n' + create_query(test.loc[i], include_label=True) for i in test.index]\n",
    "test['prompt'] = prompts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Statement: Two african women bike down a dirt road.\\nStatement: Two women bike down a road.\\nLabel: entailment\\nExplanation: Two african women bike down a dirt road women bike down a road..\\n\\nStatement: A man rides his bike on the boardwalk.\\nStatement: The boardwalk is raised off the beach.\\nLabel: neutral\\nExplanation: Not all boardwalks are raised off the beach.\\n\\nStatement: A child holding a toy while walking on a lawn.\\nStatement: The man eats the pizza\\nLabel: contradiction\\nExplanation: holding and eating are different kind of actions\\n\\nStatement: A skier wearing bright yellow pants is skiing down a snowy mountain.\\nStatement: A person doing down the mountain.\\nLabel: entailment\\nExplanation: most of the mountain are snowy\\n\\nStatement: A woman is sitting in a chair near a brick building.\\nStatement: The woman isn't sitting on the ground.\\nLabel: entailment\\nExplanation: Someone sitting in a chair isn't sitting on the ground.\\n\\nStatement: A person wearing a red coat is holding a beige and plaid umbrella while crossing the street.\\nStatement: A person is crossing the street with an umbrella.\\nLabel: entailment\\nExplanation: A person crossing the street with an umbrella is a rephrasing of the original.\\n\\nStatement: A man in a suit and dirty dress-shoes is lying on the street on pieces of cardboard with his eyes closed.\\nStatement: A man in a clean suit has gotten a new job.\\nLabel: contradiction\\nExplanation: Dirty is not clean.\\n\\nStatement: A policeman in sunglasses and uniform with other policemen.\\nStatement: Two police officers are watching a suspect from afar.\\nLabel: neutral\\nExplanation: not all policemen are watching suspects\\n\\nStatement: a dog runs down the cold aisle.\\nStatement: The dog is eating lunch\\nLabel: contradiction\\nExplanation: A dog can't be eating lunch and running at the same time.\\n\\nStatement: A man riding a snow machine topples over.\\nStatement: The snow mobile remained upright.\\nLabel: contradiction\\nExplanation: The vehicle cannot have remained upright while it topples over.\\n\\nStatement: A man giving a present to someone and about to hug the other person.\\nStatement: The man gave a present to his mother.\\nLabel: neutral\\nExplanation: Not every man has a mother.\\n\\nStatement: A man is leaning his bicycle on a tree.\\nStatement: The man had to put his bike down for a second.\\nLabel: neutral\\nExplanation: Leaning a bicycle doesn't necessarily mean the bike is put down.\\n\\nStatement: A young man in an orange hat walks his bicycle down a street.\\nStatement: A man in an orange hat walks his bike down a street.\\nLabel: \""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['prompt'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Predictions on Test Set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "pred_expl = list()\n",
    "pred_label = list()\n",
    "for i in range(int(np.ceil(test.shape[0] / 20))):\n",
    "    if i != 0:\n",
    "        time.sleep(60) # API allows only 20 calls or one call with 20 prompts per minute\n",
    "    if i*20 % 50 == 0:\n",
    "        print(i*20)\n",
    "    indices = np.arange(i * 20, (i + 1) * 20)\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=list(test.iloc[indices].prompt),\n",
    "        temperature=0.5,\n",
    "        max_tokens=58,\n",
    "        top_p=1,\n",
    "    )\n",
    "    pred_expl += [re.findall('Explanation:.+', response.choices[i].text)[0].replace('Explanation: ', '').strip() for i in range(len(response.choices))]\n",
    "    pred_label += [re.findall('.+\\nExplanation:', response.choices[i].text)[0].replace('\\nExplanation:', '').replace('Label: ', '').strip() for i in range(len(response.choices))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "test_output = test\n",
    "test_output['pred_explanation'] = pred_expl\n",
    "test_output['pred_label'] = pred_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test_output.to_csv('../output_data/test_predictions_4examples.csv', sep=';')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.675"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_output.gold_label == test_output.pred_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_label     assumption  contradiction  entailment  equivalence  equivalent  \\\n",
      "gold_label                                                                      \n",
      "contradiction           0            299           5            0           0   \n",
      "entailment              0              2         332            3           1   \n",
      "neutral                 1             73         146            1           1   \n",
      "\n",
      "pred_label     implication  inference  neutral  partial entailment  \\\n",
      "gold_label                                                           \n",
      "contradiction            0          0       27                   0   \n",
      "entailment               0          0       16                   1   \n",
      "neutral                  2          4       44                   0   \n",
      "\n",
      "pred_label     redundancy  reinforcement  repetition  speculation  synonym  \\\n",
      "gold_label                                                                   \n",
      "contradiction           0              0           0            1        0   \n",
      "entailment              1              1           1            1        3   \n",
      "neutral                 0              0           0           27        0   \n",
      "\n",
      "pred_label     tautology  unrelated  \n",
      "gold_label                           \n",
      "contradiction          0          4  \n",
      "entailment             3          0  \n",
      "neutral                0          0  \n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(test_output.gold_label, test_output.pred_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
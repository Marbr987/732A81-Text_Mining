{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate Explanations\n",
    "\n",
    "The code in this notebook is for evaluating the predictions produced by GPT-3 based on the metrics suggested by Kunz, Human Ratings do not Reflect Downstream Utility, 2022."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import time\n",
    "from scripts.prepare_data_helpers import prepare_examples, create_query\n",
    "import spacy\n",
    "import textacy\n",
    "from dotenv import load_dotenv\n",
    "from bert_score import score\n",
    "import transformers\n",
    "\n",
    "load_dotenv()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "import xgboost as xgb\n",
    "import shap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('../../e-SNLI/dataset/esnli_train_1.csv')\n",
    "train2 = pd.read_csv('../../e-SNLI/dataset/esnli_train_2.csv')\n",
    "train = pd.concat([train1, train2])\n",
    "dev = pd.read_csv('../../e-SNLI/dataset/esnli_dev.csv')\n",
    "test = pd.read_csv('../../e-SNLI/dataset/esnli_test.csv')\n",
    "\n",
    "train = train.dropna(subset=['Sentence1', 'Sentence2', 'Explanation_1'])\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "np.random.seed(12345) # seed for numpy package\n",
    "test_indices = list(np.random.choice(test.index.values, size=1000, replace=False))\n",
    "test = test.loc[test_indices]\n",
    "test = test.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "gpt_pred = pd.read_csv('../output_data/svo_structure_4.csv', sep=';')\n",
    "# Transform list to single string\n",
    "gpt_pred.reduced_expl = [i.replace(\"'\", \" \").replace(\" \", \"\").strip(\"][\").replace(\",\", \" \") for i in gpt_pred.reduced_expl]\n",
    "gpt_pred.pos_expl = [i.replace(\"'\", \" \").replace(\" \", \"\").strip(\"][\").replace(\",\", \" \") for i in gpt_pred.pos_expl]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert Score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:17<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 75.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 77.94 seconds, 12.83 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score(list(gpt_pred.pred_explanation), list(gpt_pred.Explanation_1), lang=\"en\", verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Surface Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words predicted explanations 14.382\n",
      "Average number of words gold explanations 12.916\n",
      " \n",
      "Average number of characters predicted explanations 76.974\n",
      "Average number of characters gold explanations 67.122\n"
     ]
    }
   ],
   "source": [
    "nwords = gpt_pred.pred_explanation.apply(lambda x: len(x.split()))\n",
    "nwords_gold = gpt_pred.Explanation_1.apply(lambda x: len(x.split()))\n",
    "\n",
    "nchars = gpt_pred.pred_explanation.apply(len)\n",
    "nchars_gold = gpt_pred.Explanation_1.apply(len)\n",
    "\n",
    "print(f\"Average number of words predicted explanations {nwords.mean()}\")\n",
    "print(f\"Average number of words gold explanations {nwords_gold.mean()}\")\n",
    "print(\" \")\n",
    "\n",
    "print(f\"Average number of characters predicted explanations {nchars.mean()}\")\n",
    "print(f\"Average number of characters gold explanations {nchars_gold.mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "pred_expl_vocab = gpt_pred.pred_explanation.apply(lambda x: np.unique([token.lemma_ for token in nlp(x) if not token.is_punct]))\n",
    "Expl1_vocab = gpt_pred.Explanation_1.apply(lambda x: np.unique([token.lemma_ for token in nlp(x) if not token.is_punct]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "overlap = list()\n",
    "for pred, gold in zip(pred_expl_vocab, Expl1_vocab):\n",
    "    overlap.append(len(set(pred) & set(gold)) / len(pred))\n",
    "overlap = np.array(overlap)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "0.47741691306320316"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size predicted explanations: 1664\n",
      "Total vocabulary size gold explanations: 1616\n",
      "Total overlap: 0.7379807692307693\n"
     ]
    }
   ],
   "source": [
    "pred_expl_vocab_total = set([x for l in pred_expl_vocab for x in l])\n",
    "Expl1_vocab_total = set([x for l in Expl1_vocab for x in l])\n",
    "overlap_total = len(pred_expl_vocab_total & Expl1_vocab_total) / len(pred_expl_vocab_total)\n",
    "print(f\"Total vocabulary size predicted explanations: {len(pred_expl_vocab_total)}\")\n",
    "print(f\"Total vocabulary size gold explanations: {len(Expl1_vocab_total)}\")\n",
    "print(f\"Total overlap: {overlap_total}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "0.79"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gpt_pred.pred_label == gpt_pred.gold_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}